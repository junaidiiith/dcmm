{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_edges_in_undirected_graph(graph):\n",
    "    edges = set()\n",
    "    for edge in graph.edges():\n",
    "        if (edge[0] != edge[1]) and (edge[0], edge[1]) not in edges and (edge[1], edge[0]) not in edges:\n",
    "            edges.add(edge)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "def get_coupling(graph, partition):\n",
    "    clusters = list(set(partition.values()))\n",
    "    edges_between_clusters = list()\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(i+1, len(clusters)):\n",
    "            cluster1, cluster2 = clusters[i], clusters[j]\n",
    "            sub_graph1 = graph.subgraph([node for node in partition if partition[node] == cluster1])\n",
    "            sub_graph2 = graph.subgraph([node for node in partition if partition[node] == cluster2])\n",
    "            edges_between_clusters += list(nx.edge_boundary(graph, sub_graph1, sub_graph2))\n",
    "\n",
    "    coupling = len(edges_between_clusters) /  graph.number_of_edges()\n",
    "    return coupling\n",
    "\n",
    "\n",
    "def get_cohesion(graph, partition):\n",
    "    clusters = set(partition.values())\n",
    "    # print(len(clusters))\n",
    "    cohesion = 0\n",
    "    for cluster in clusters:\n",
    "        sub_graph = graph.subgraph([node for node in partition if partition[node] == cluster])\n",
    "        max_edges = sub_graph.number_of_nodes() * (sub_graph.number_of_nodes() - 1) / 2\n",
    "        edges = get_edges_in_undirected_graph(sub_graph)\n",
    "        cohesion += len(edges) / max_edges if max_edges != 0 else 0\n",
    "\n",
    "    return cohesion / len(clusters)\n",
    "\n",
    "\n",
    "def get_modularization_scores(graph, partition):\n",
    "    scores = {\n",
    "        'cohesion': get_cohesion(graph, partition),\n",
    "        'coupling': get_coupling(graph, partition),\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ga_results = pd.read_excel('results/GA2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import networkx as nx\n",
    "\n",
    "with open('dataset/ecore_non_dup_models.pkl', 'rb') as f:\n",
    "    non_duplicate_models = pickle.load(f)\n",
    "\n",
    "non_duplicate_numbered_graphs = [(a, nx.convert_node_labels_to_integers(b)) for a, b in non_duplicate_models if list(nx.isolates(b)) == []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 91)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top50_graphs = sorted(non_duplicate_numbered_graphs, key=lambda x: x[1].number_of_edges(), reverse=True)[:50]\n",
    "file_name, graph = non_duplicate_numbered_graphs[0]\n",
    "graph.number_of_nodes(), graph.number_of_edges()\n",
    "\n",
    "# graph = remove_isolated_nodes(nxg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max cohesion row: 0.604444444444444, 0.0\n",
      "Min coupling row: 0.503703703703703, 0.0\n"
     ]
    }
   ],
   "source": [
    "prefix = '../datasets/ModelClassification/modelset/raw-data/repo-ecore-all/data/'\n",
    "\n",
    "def get_file_name(f):\n",
    "    return f.replace(prefix, '').replace('.ecore', '').replace('/', '_')\n",
    "\n",
    "file_name = get_file_name(file_name)\n",
    "\n",
    "file_df = ga_results[ga_results['Name'] == file_name]\n",
    "max_cohesion_row = dict(file_df.loc[file_df['Cohesion'].idxmax()])\n",
    "min_coupling_row = dict(file_df.loc[file_df['Coupling'].idxmin()])\n",
    "\n",
    "print(f\"Max cohesion row: {max_cohesion_row['Cohesion']}, {max_cohesion_row['Coupling']}\")\n",
    "print(f\"Min coupling row: {min_coupling_row['Cohesion']}, {min_coupling_row['Coupling']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paulofpimenta_b-reactive_org.cirad.dsl.behaviormetamodel_model_behaviormetamodel'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "def get_nx_adj(nxg) -> torch.Tensor:\n",
    "    adj = nx.adjacency_matrix(nxg).todense()\n",
    "    return torch.tensor(adj, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_index(nxg):\n",
    "    edge_index = torch.tensor(list(nxg.edges)).t().contiguous()\n",
    "    return edge_index\n",
    "\n",
    "edge_index = get_edge_index(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sali/Miniconda/miniconda3/envs/ML/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "NODE2VEC_EPOCHS = 1\n",
    "NODE2VEC_WALK_LENGTH = 10\n",
    "NODE2VEC_CONTEXT_SIZE = 10\n",
    "NODE2VEC_DIM = 128\n",
    "NODE2VEC_NEG_SAMPLES = 4\n",
    "NODE2VEC_BATCH_SIZE = 128\n",
    "NODE2VEC_LR = 0.01\n",
    "NODE2VEC_WALKS_PER_NODE = 10\n",
    "NODE2VEC_NUM_WORKERS = 4\n",
    "NODE2VEC_P = 0.8\n",
    "NODE2VEC_Q = 1.2\n",
    "\n",
    "def get_node_embeddings(g):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    edge_index = get_edge_index(g)\n",
    "    node2vec = Node2Vec(\n",
    "        edge_index,\n",
    "        embedding_dim=NODE2VEC_DIM,\n",
    "        walk_length=NODE2VEC_WALK_LENGTH,\n",
    "        context_size=NODE2VEC_CONTEXT_SIZE,\n",
    "        walks_per_node=NODE2VEC_WALKS_PER_NODE,\n",
    "        num_negative_samples=NODE2VEC_NEG_SAMPLES,\n",
    "        p=NODE2VEC_P,\n",
    "        q=NODE2VEC_Q,\n",
    "        sparse=True,\n",
    "    ).to(device)\n",
    "\n",
    "    loader = node2vec.loader(batch_size=NODE2VEC_BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    optimizer = torch.optim.SparseAdam(list(node2vec.parameters()), lr=NODE2VEC_LR)\n",
    "    node2vec.train()\n",
    "    total_loss = 0\n",
    "    for epoch in tqdm(range(1, NODE2VEC_EPOCHS + 1), desc='Training Node2Vec For Node Embeddings'):\n",
    "        total_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = node2vec.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        loss = total_loss / len(loader)\n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "\n",
    "    return node2vec.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GNNModel(torch.nn.Module):\n",
    "    \"\"\"GNN Model with multiple layers\"\"\"\n",
    "    def __init__(self, model_name, input_dim, hidden_dim, out_dim, num_layers, num_heads=None, residual=False, l_norm=False, dropout=0.1):\n",
    "        super(GNNModel, self).__init__()\n",
    "        gnn_model = getattr(torch_geometric.nn, model_name)\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        if model_name == 'GINConv':\n",
    "            input_layer = gnn_model(nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU()), train_eps=True)\n",
    "        elif num_heads is None:\n",
    "            input_layer = gnn_model(input_dim, hidden_dim, aggr='SumAggregation')\n",
    "        else:\n",
    "            input_layer = gnn_model(input_dim, hidden_dim, heads=num_heads, aggr='SumAggregation')\n",
    "        self.conv_layers.append(input_layer)\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            if model_name == 'GINConv':\n",
    "                self.conv_layers.append(gnn_model(nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU()), train_eps=True))\n",
    "            elif num_heads is None:\n",
    "                self.conv_layers.append(gnn_model(hidden_dim, hidden_dim, aggr='SumAggregation'))\n",
    "            else:\n",
    "                self.conv_layers.append(gnn_model(num_heads*hidden_dim, hidden_dim, heads=num_heads, aggr='SumAggregation'))\n",
    "\n",
    "        if model_name == 'GINConv':\n",
    "            self.conv_layers.append(gnn_model(nn.Sequential(nn.Linear(hidden_dim, out_dim), nn.ReLU()), train_eps=True))\n",
    "        else:\n",
    "            self.conv_layers.append(gnn_model(hidden_dim if num_heads is None else num_heads*hidden_dim, out_dim, aggr='SumAggregation'))\n",
    "            \n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim if num_heads is None else num_heads*hidden_dim) if l_norm else None\n",
    "        self.residual = residual\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, in_feat, edge_index):\n",
    "        h = in_feat\n",
    "        h = self.conv_layers[0](h, edge_index)\n",
    "        h = self.activation(h)\n",
    "        if self.layer_norm is not None:\n",
    "            h = self.layer_norm(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        for conv in self.conv_layers[1:-1]:\n",
    "            h = conv(h, edge_index) if not self.residual else conv(h, edge_index) + h\n",
    "            h = self.activation(h)\n",
    "            if self.layer_norm is not None:\n",
    "                h = self.layer_norm(h)\n",
    "            h = self.dropout(h)\n",
    "        \n",
    "        h = self.conv_layers[-1](h, edge_index)\n",
    "        return h\n",
    "# Y = model(X, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphClusteringLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GraphClusteringLoss, self).__init__()\n",
    "        # Initialize lambda as a learnable parameter\n",
    "        self.lambda_param = nn.Parameter(torch.tensor(0.5))  # Initial value of lambda\n",
    "\n",
    "\n",
    "    def forward(self, A: torch.Tensor, Y: torch.Tensor):\n",
    "        n, C = Y.shape\n",
    "\n",
    "        Y = F.gumbel_softmax(Y, dim=1)  # Shape: n x C\n",
    "        \n",
    "        # Compute cohesion\n",
    "        cohesion = 0\n",
    "        for c in range(C):\n",
    "            Y_c = Y[:, c].unsqueeze(1)  # Shape: n x 1\n",
    "            A_c = Y_c @ Y_c.T  # Shape: n x n\n",
    "            A_c = A_c * A  # Mask with adjacency matrix to get within-cluster edges\n",
    "            total_edges_within_cluster = A_c.sum()\n",
    "            \n",
    "            n_c = Y[:, c].sum()\n",
    "            max_possible_edges_within_cluster = n_c * (n_c - 1) / 2 + 1e-9  # Avoid division by zero\n",
    "            cohesion += total_edges_within_cluster / max_possible_edges_within_cluster\n",
    "        \n",
    "        # cohesion /= C  # Average cohesion across all clusters\n",
    "        print(f'Cohesion with loops: {cohesion.item()}')\n",
    "\n",
    "        # Compute cohesion\n",
    "        Y_YT = torch.einsum('ni, nj->nij', Y, Y)  # Shape: n x n x C\n",
    "        A_expanded = A.unsqueeze(2)  # Shape: n x n x 1\n",
    "        print(A_expanded.shape, Y_YT.shape)\n",
    "\n",
    "        # Mask with adjacency matrix to get within-cluster edges\n",
    "        A_Y_YT = A_expanded * Y_YT  # Shape: n x n x C\n",
    "        total_edges_within_cluster = A_Y_YT.sum(dim=(0, 1))  # Shape: C\n",
    "\n",
    "        # Number of nodes in each cluster\n",
    "        n_c = Y.sum(dim=0)  # Shape: C\n",
    "\n",
    "        # Max possible edges within each cluster\n",
    "        max_possible_edges_within_cluster = n_c * (n_c - 1) / 2 + 1e-9  # Shape: C\n",
    "\n",
    "        # Cohesion for each cluster\n",
    "        cohesion_per_cluster = total_edges_within_cluster / max_possible_edges_within_cluster  # Shape: C\n",
    "\n",
    "        # Average cohesion across all clusters\n",
    "        cohesion = cohesion_per_cluster.sum()\n",
    "\n",
    "        print(f'Cohesion without loops: {cohesion.item()}')\n",
    "\n",
    "        total_edges = A.sum()\n",
    "\n",
    "        inter_cluster_edges = torch.zeros(1).to(A.device)\n",
    "        \n",
    "        for i in range(C):\n",
    "            for j in range(C):\n",
    "                if i != j:\n",
    "                    Y_i = Y[:, i].unsqueeze(1)\n",
    "                    Y_j = Y[:, j].unsqueeze(1)\n",
    "                    A_ij = Y_i @ Y_j.T\n",
    "                    inter_cluster_edges += (A_ij * A).sum()\n",
    "\n",
    "\n",
    "        assert inter_cluster_edges <= total_edges, f'Inter-cluster edges cannot be greater than total edges in the graph. Inter-cluster edges: {inter_cluster_edges}, Total edges: {total_edges}'\n",
    "        coupling = inter_cluster_edges / (total_edges + 1e-9)  # Avoid division by zero\n",
    "\n",
    "        print(f'Coupling with loops: {coupling.item()}')\n",
    "\n",
    "        # Calculate the cluster probabilities matrix product\n",
    "        Y_YT = Y @ Y.T  # Shape: n x n\n",
    "\n",
    "        # Mask the intra-cluster edges\n",
    "        intra_cluster_mask = torch.eye(n, device=A.device).bool()\n",
    "        Y_YT[intra_cluster_mask] = 0\n",
    "\n",
    "        # Compute inter-cluster edges\n",
    "        inter_cluster_edges = (Y_YT * A).sum() / 2  # Divide by 2 to avoid double counting\n",
    "        coupling = inter_cluster_edges / (total_edges + 1e-9)  # Avoid division by zero\n",
    "\n",
    "        print(f'Coupling without loops: {coupling.item()}')\n",
    "\n",
    "        # Calculate loss\n",
    "        # loss = -cohesion\n",
    "        loss = -cohesion * self.lambda_param + (1 - self.lambda_param) * coupling\n",
    "\n",
    "        node_clusters = torch.argmax(Y, dim=1).cpu().numpy()\n",
    "        clusters = {i: c.item() for i, c in enumerate(node_clusters)}\n",
    "        g = nx.from_numpy_array(A.detach().cpu().numpy())\n",
    "        # metrics = get_modularization_scores(g, clusters)\n",
    "\n",
    "        # if settings.verbose:\n",
    "        print(f'Loss: {loss.item():.4f}, Cohesion: {cohesion.item()/C:.4f}, Coupling: {coupling.item()/total_edges:.4f}, Lambda: {self.lambda_param.item():.4f}')\n",
    "        # print(f'Loss: {loss.item():.4f}, Cohesion: {cohesion.item()/C:.4f}')\n",
    "        # print(f'Actual Cohesion: {metrics[\"cohesion\"]:.4f}, Actual Coupling: {metrics[\"coupling\"]:.4f}, Clusters: {len(set(clusters.values()))}')\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data(g):\n",
    "    X = get_node_embeddings(g).float()\n",
    "    A = get_nx_adj(g).float()\n",
    "    E = get_edge_index(g)\n",
    "    print(f\"Node embeddings shape: {X.shape}\")\n",
    "    print(f\"Adjacency matrix shape: {A.shape}\")\n",
    "    print(f\"Edge index shape: {E.shape}\")\n",
    "    return X, E, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Node2Vec For Node Embeddings: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n"
     ]
    }
   ],
   "source": [
    "X = get_node_embeddings(graph).float()\n",
    "A = get_nx_adj(graph).float()\n",
    "E = get_edge_index(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for n in graph.nodes if graph.degree(n) == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNN_NUM_EPOCHS = 1\n",
    "GNN_MODEL_NAME = 'SAGEConv'\n",
    "GNN_INPUT_DIM = NODE2VEC_DIM\n",
    "GNN_HIDDEN_DIM = 128\n",
    "GNN_NUM_LAYERS = 3\n",
    "GNN_RESIDUAL = True\n",
    "GNN_LNORM = True\n",
    "GNN_DROPOUT = 0.1\n",
    "GNN_LR = 0.01\n",
    "NODES_PER_CLUSTER = 7\n",
    "\n",
    "\n",
    "def train_gnn(g):\n",
    "    loss_fn = GraphClusteringLoss()\n",
    "    X, edge_index, A = get_input_data(g)\n",
    "    model = GNNModel(\n",
    "        model_name=GNN_MODEL_NAME, \n",
    "        input_dim=GNN_INPUT_DIM, \n",
    "        hidden_dim=GNN_HIDDEN_DIM, \n",
    "        out_dim=g.number_of_nodes() // NODES_PER_CLUSTER, \n",
    "        num_layers=GNN_NUM_LAYERS, \n",
    "        residual=GNN_RESIDUAL, \n",
    "        l_norm=GNN_LNORM, \n",
    "        dropout=GNN_DROPOUT\n",
    "    ).to(device)\n",
    "\n",
    "    ## loss_fn.lambda_param + model.parameters\n",
    "    train_params = list(model.parameters()) + [loss_fn.lambda_param]\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(train_params, lr=GNN_LR)\n",
    "    all_metrics = list()\n",
    "    for epoch in tqdm(range(1, GNN_NUM_EPOCHS + 1), desc='GNN Epochs'):\n",
    "        optimizer.zero_grad()\n",
    "        Y = model(X, edge_index)\n",
    "        loss = loss_fn(A, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        node_clusters = torch.argmax(Y, dim=1).cpu()\n",
    "        node_to_cluster_map = {i: c.item() for i, c in enumerate(node_clusters)}\n",
    "        metrics = get_modularization_scores(graph, node_to_cluster_map)\n",
    "        all_metrics.append(metrics)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Cohesion: {metrics[\"cohesion\"]:.4f}, Coupling: {metrics[\"coupling\"]:.4f}')\n",
    "            print(f\"Lambda Value: {loss_fn.lambda_param.item()}\")\n",
    "    \n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gnn(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_models.dmon import Single\n",
    "\n",
    "DMON_EPOCHS = 1\n",
    "DMON_LR = 0.001\n",
    "\n",
    "\n",
    "def train_dmon(g):\n",
    "    X, _, A = get_input_data(g)\n",
    "    ips = (X.unsqueeze(0), A.unsqueeze(0))\n",
    "    model = Single(\n",
    "        X.shape[1], \n",
    "        len(g.nodes)//NODES_PER_CLUSTER, \n",
    "        skip_conn=False, \n",
    "        collapse_regularization=0.1\n",
    "    )\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=DMON_LR)\n",
    "\n",
    "    model.train()\n",
    "    all_metrics = list()\n",
    "    losses_list, mod_scores = list(), list()\n",
    "    for _ in tqdm(range(DMON_EPOCHS), desc='DMoN Epochs'):\n",
    "        optimizer.zero_grad()\n",
    "        _, pred, _, losses = model(ips)\n",
    "        loss = torch.FloatTensor([0]).to(device)\n",
    "        for loss_val in losses.values():\n",
    "            if loss_val is not None:\n",
    "                loss += loss_val\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if epoch % 1000 == 0:\n",
    "        #     print(f\"Epoch: {epoch}, Loss: {loss.item()}\\n\")\n",
    "        losses_list.append(loss.item())\n",
    "        dmon_clusters = {node: cluster for node, cluster in zip(\n",
    "        g.nodes(), pred[0].detach().cpu().numpy().argmax(axis=-1))}\n",
    "        \n",
    "        metrics = get_modularization_scores(g, dmon_clusters)\n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc8baac42944963a0049dde188dfe44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Node2Vec For Node Embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605e1bca78314d9abf15b7bf659e7a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DMoN Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'cohesion': 0.07680152112583719, 'coupling': 0.6771907216494846}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dmon(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_models.DGI import DeepGraphInfomax, Encoder, Summarizer, corruption, cluster_net\n",
    "\n",
    "DGI_EPOCHS = 1\n",
    "DGI_LR = 0.001\n",
    "DGI_HIDDEN_DIM = 128\n",
    "\n",
    "def make_modularity_matrix(adj):\n",
    "    adj = adj*(torch.ones(adj.shape[0], adj.shape[0]).to(device) - torch.eye(adj.shape[0]).to(device))\n",
    "    degrees = adj.sum(dim=0).unsqueeze(1)\n",
    "    mod = adj - degrees@degrees.t()/adj.sum()\n",
    "    return mod\n",
    "\n",
    "\n",
    "def train_dgi(g):\n",
    "    X = get_node_embeddings(g)\n",
    "    edge_index = get_edge_index(g)\n",
    "    adj = get_nx_adj(g)\n",
    "     \n",
    "    model = DeepGraphInfomax(\n",
    "        hidden_channels=DGI_HIDDEN_DIM, \n",
    "        encoder=Encoder(X.shape[1], DGI_HIDDEN_DIM),\n",
    "        out_channels=len(g.nodes)//NODES_PER_CLUSTER,\n",
    "        summary=Summarizer(),\n",
    "        corruption=corruption,\n",
    "        cluster=cluster_net)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=DGI_LR, weight_decay=5e-3)\n",
    "\n",
    "    adj = adj.float().to(device)\n",
    "    mod = make_modularity_matrix(adj)\n",
    "    model.train()\n",
    "    epoch_losses = list()\n",
    "    all_metrics = list()\n",
    "\n",
    "    for _ in tqdm(range(DGI_EPOCHS), desc='DGI Epochs'):\n",
    "        optimizer.zero_grad()\n",
    "        pos_z, neg_z, summary, mu, r, _ = model(X, edge_index)\n",
    "        dgi_loss = model.loss(pos_z, neg_z, summary)\n",
    "        modularity_loss = model.modularity(r, adj, mod)\n",
    "        comm_loss = model.comm_loss(pos_z, mu)\n",
    "        # loss = -modularity_loss\n",
    "        loss = 5*dgi_loss - modularity_loss + comm_loss\n",
    "\n",
    "        # print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        commdgi_clusters = {node: cluster for node, cluster in zip(\n",
    "        g.nodes(), r.detach().cpu().numpy().argmax(axis=-1))}\n",
    "\n",
    "        metrics = get_modularization_scores(g, commdgi_clusters)\n",
    "        all_metrics.append(metrics)        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c68ce7243454a75b775d92bdf502a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Node2Vec For Node Embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d954aa30552340fca0826288eacf923e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DGI Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'cohesion': 0.3125866964576641, 'coupling': 0.8582474226804123}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dgi(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42504, 8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "raw_ga = pd.read_excel('results/GA2.xlsx')\n",
    "raw_ga.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42294, 8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_cols = raw_ga['Coupling'].isna()\n",
    "### remove rows with na_cols\n",
    "\n",
    "raw_ga = raw_ga[~na_cols]\n",
    "raw_ga.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = pd.read_excel('results/all_gnn_results.xlsx')\n",
    "gnn['file_name'] = gnn['file_name'].apply(lambda x: x.replace(\"../datasets/ModelClassification/modelset/raw-data/repo-ecore-all/data/\", \"\").replace('.ecore', \"\").replace(\"/\", '_'))\n",
    "gnn['metrics'] = gnn['metrics'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ga_vs_gnn_files = list(set(gnn['file_name']).intersection(set(raw_ga['Name'])))\n",
    "len(ga_vs_gnn_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15e4665dfbe4fc18f283a615eb185bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ast\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ga_rows = list()\n",
    "for ga_vs_gnn_file in tqdm(ga_vs_gnn_files):\n",
    "    d = {'file_name': ga_vs_gnn_file}\n",
    "    solution_paretos = list()\n",
    "    ga_results = raw_ga[raw_ga['Name'] == ga_vs_gnn_file]\n",
    "    paretos = list(set(ga_results['Pareto'].values))\n",
    "    for pareto in paretos:\n",
    "        solutions = list()\n",
    "        instances = ga_results[ga_results['Pareto'] == pareto]\n",
    "        for i, r in instances.iterrows():\n",
    "            solutions.append({\n",
    "                'cohesion': r['Cohesion'],\n",
    "                'coupling': r['Coupling'],\n",
    "                # 'clusters': ast.literal_eval(r['String'])\n",
    "            })\n",
    "    \n",
    "        solution_paretos.append(solutions)\n",
    "    d['metrics'] = solution_paretos\n",
    "    ga_rows.append(d)\n",
    "\n",
    "pd.DataFrame(ga_rows).to_excel('results/all_ga_results.xlsx', index=False)\n",
    "ga = pd.read_excel('results/all_ga_results.xlsx')\n",
    "ga['metrics'] = ga['metrics'].apply(lambda x: ast.literal_eval(x.replace('nan', '\\\"nan\\\"')))\n",
    "ga_f2metrics = {r['file_name']: r['metrics'] for i, r in ga.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cohesion': 0.549842638204707, 'coupling': 0.214285714285714},\n",
       " {'cohesion': 0.0586712138436276, 'coupling': 0.0},\n",
       " {'cohesion': 0.374623078071354, 'coupling': 0.0714285714285714},\n",
       " {'cohesion': 0.389520824003582, 'coupling': 0.142857142857142}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ga_f2metrics['47hm4n3_idmDrone_plugin_sar.drone_model_generated_Drn'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_results_dict(gnn_results, ga_file_map):\n",
    "    combined_results = dict()\n",
    "    for _, r in gnn_results.iterrows():\n",
    "        file_name = r['file_name']\n",
    "        if file_name not in ga_file_map:\n",
    "            continue\n",
    "        ga_metrics = ga_file_map[file_name]\n",
    "        gnn_metrics = r['metrics']\n",
    "        gnn_metrics['ga'] = ga_metrics\n",
    "        combined_results[file_name] = gnn_metrics\n",
    "    \n",
    "    print(f\"Combined results length: {len(combined_results)}\")\n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pareto_area(pareto_result):\n",
    "    def get_area(pareto_points):\n",
    "        area = 0.0\n",
    "        for i in range(len(pareto_points)):\n",
    "            x1, y1 = pareto_points[i]\n",
    "            if i  == 0:\n",
    "                a = x1*y1\n",
    "            else:\n",
    "                x2, _ = pareto_points[i-1]\n",
    "                a = (x1 - x2) * y1\n",
    "            area += a\n",
    "        return area\n",
    "    points = list()\n",
    "    for d in pareto_result:\n",
    "        cohesion = d['cohesion'] if isinstance(d['cohesion'], float) else 0\n",
    "        coupling = d['coupling'] if isinstance(d['coupling'], float) else 1\n",
    "        points.append((cohesion, 1 - coupling))\n",
    "    points = sorted(points, key=lambda x: x[0])\n",
    "    return get_area(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gnn_area(gnn_result):\n",
    "    max_cohesion = gnn_result['max_cohesion']\n",
    "    min_coupling = gnn_result['min_coupling']\n",
    "    max_cohesion_area =  max_cohesion['cohesion'] * (1 - max_cohesion['coupling'])\n",
    "    min_coupling_area = min_coupling['cohesion'] * (1 - min_coupling['coupling'])\n",
    "    # print(max_cohesion['cohesion'], max_cohesion['coupling'])\n",
    "    # print(min_coupling['cohesion'], min_coupling['coupling'])\n",
    "    # print('---')\n",
    "    return max(max_cohesion_area, min_coupling_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gnn_max_cohesion(gnn_result):\n",
    "    score = gnn_result['max_cohesion']['cohesion'] \n",
    "    return score if score != 'nan' else 0\n",
    "\n",
    "\n",
    "def get_gnn_min_coupling(gnn_result):\n",
    "    score = gnn_result['max_cohesion']['coupling']\n",
    "    return score if score != 'nan' else 1\n",
    "\n",
    "def get_max_area_pareto(ga_result):\n",
    "    max_area = 0\n",
    "    best_instance = None\n",
    "    for pareto in ga_result:\n",
    "        for instance in pareto:\n",
    "            if instance['cohesion'] == 'nan' or instance['coupling'] == 'nan':\n",
    "                area = 0\n",
    "            else:\n",
    "                area = instance['cohesion'] * (1 - instance['coupling'])\n",
    "            if area > max_area:\n",
    "                max_area = area\n",
    "                best_instance = instance\n",
    "\n",
    "    return best_instance\n",
    "\n",
    "def get_ga_max_cohesion(ga_result):\n",
    "    best_instance = get_max_area_pareto(ga_result)\n",
    "    return best_instance['cohesion']\n",
    "\n",
    "\n",
    "def get_ga_min_coupling(ga_result):\n",
    "    best_instance = get_max_area_pareto(ga_result)\n",
    "    return best_instance['coupling']\n",
    "\n",
    "\n",
    "def get_overall_max_cohesion(result):\n",
    "    max_gnn_cohesion = max([(get_gnn_max_cohesion(result[k]), k) for k in ['dgi', 'dmon', 'gnn']])\n",
    "    max_ga_cohesion = (get_ga_max_cohesion(result['ga']), 'ga')\n",
    "    max_overall_cohesion, max_overall_cohesion_algo = max(max_gnn_cohesion, max_ga_cohesion)\n",
    "    max_gnn_cohesion_score, max_gnn_cohesion_algo = max_gnn_cohesion\n",
    "    return max_overall_cohesion, max_overall_cohesion_algo, max_gnn_cohesion_score, max_gnn_cohesion_algo\n",
    "\n",
    "def get_overall_min_coupling(result):\n",
    "    min_gnn_coupling = min([(get_gnn_min_coupling(result[k]), k) for k in ['dgi', 'dmon', 'gnn']])\n",
    "    min_ga_coupling = (get_ga_min_coupling(result['ga']), 'ga')\n",
    "    min_overall_coupling, min_overall_coupling_algo = min(min_gnn_coupling, min_ga_coupling)\n",
    "    min_gnn_coupling_score, min_gnn_coupling_algo = min_gnn_coupling\n",
    "    return min_overall_coupling, min_overall_coupling_algo, min_gnn_coupling_score, min_gnn_coupling_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_all_results_metrics(all_results):\n",
    "    rows = list()\n",
    "    for file_name, model_results in all_results.items():\n",
    "        ga_areas = [get_pareto_area(p) for p in model_results['ga']]\n",
    "        mean_ga = (sum(ga_areas)/len(ga_areas), 'ga')\n",
    "        max_ga = (max(ga_areas), 'ga')\n",
    "        max_ga_area = max_ga[0]\n",
    "\n",
    "        dgi_area = (get_gnn_area(model_results['dgi']), 'dgi')\n",
    "        dmon_area = (get_gnn_area(model_results['dmon']), 'dmon')\n",
    "        gnn_area = (get_gnn_area(model_results['gnn']), 'gnn')\n",
    "\n",
    "        dgi_hp_score = (np.sum([dgi_area[0] >= ga_area for ga_area in ga_areas]), 'dgi')\n",
    "        dmon_hp_score = (np.sum([dmon_area[0] >= ga_area for ga_area in ga_areas]), 'dmon')\n",
    "        gnn_hp_score = (np.sum([gnn_area[0] >= ga_area for ga_area in ga_areas]), 'gnn')\n",
    "        best_hp_score, best_hp_algo = max([dgi_hp_score, dmon_hp_score, gnn_hp_score])\n",
    "        best_hp_algo = best_hp_algo if best_hp_score / len(ga_areas) >= 0.5 else 'ga'\n",
    "        \n",
    "        \n",
    "        max_gnn = max([dgi_area, dmon_area, gnn_area])\n",
    "        max_gnn_area, max_gnn_area_algo = max_gnn\n",
    "        max_overall_area, max_overall_area_aglo = max(max_ga, max_gnn)\n",
    "        mean_overall_area, mean_overall_area_algo = max(mean_ga, max_gnn)\n",
    "        \n",
    "        max_overall_cohesion, max_overall_cohesion_algo, max_gnn_cohesion, max_gnn_cohesion_algo = get_overall_max_cohesion(model_results)\n",
    "        min_overall_coupling, min_overall_coupling_algo, min_gnn_coupling, min_gnn_coupling_algo = get_overall_min_coupling(model_results)\n",
    "\n",
    "        rows.append({\n",
    "            'file_name': file_name,\n",
    "            'max_ga_area': max(ga_areas),\n",
    "            'dgi_area': dgi_area[0],\n",
    "            'dmon_area': dmon_area[0],\n",
    "            'gnn_area': gnn_area[0],\n",
    "            'Max Hypervolume GNN': max_gnn_area,\n",
    "            'Max Hypervolume GNN Model': max_gnn_area_algo,\n",
    "\n",
    "            'max_ga_area': max_ga_area,\n",
    "            \n",
    "            'Max Hypervolume': max_overall_area,\n",
    "            'Max Hypervolume Model': max_overall_area_aglo,\n",
    "            'Avg. Hypervolume': mean_overall_area,\n",
    "            'Avg. Hypervolume Model': mean_overall_area_algo,\n",
    "\n",
    "            'Max Cohesion': max_overall_cohesion,\n",
    "            'Max Cohesion Model': max_overall_cohesion_algo,\n",
    "            'Max Cohesion (GNNs)': max_gnn_cohesion,\n",
    "            'Max Cohesion (GNNs) Model': max_gnn_cohesion_algo,\n",
    "\n",
    "            'Min Coupling': min_overall_coupling,\n",
    "            'Min Coupling Model': min_overall_coupling_algo,\n",
    "            'Min Coupling (GNNs)': min_gnn_coupling,\n",
    "            'Min Coupling (GNNs) Model': min_gnn_coupling_algo,\n",
    "\n",
    "            'Best Hypervolume Score': best_hp_score,\n",
    "            'Best Hypervolume Score (Norm)': best_hp_score / len(ga_areas),\n",
    "            'Best Hypervolume Model': best_hp_algo,\n",
    "        })\n",
    "    \n",
    "    final_result = pd.DataFrame(rows)\n",
    "    final_result.to_excel('results/all_results.xlsx', index=False)\n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def get_results_df(f):\n",
    "    df = pd.read_excel(f)\n",
    "    df['file_name'] = df['file_name'].apply(lambda x: x.replace(\"../datasets/ModelClassification/modelset/raw-data/repo-ecore-all/data/\", \"\").replace('.ecore', \"\").replace(\"/\", '_'))\n",
    "    df['metrics'] = df['metrics'].apply(lambda x: ast.literal_eval(x))\n",
    "    if isinstance(df.iloc[0]['metrics'], list):\n",
    "        df['metrics'] = [r['metrics'][r['run']] for i, r in df.iterrows()]\n",
    "    return df\n",
    "\n",
    "large_models = get_results_df('results/allmax_-1_min_101_results.xlsx')\n",
    "medium_models = get_results_df('results/allmax_100_min_50_results.xlsx')\n",
    "small_models = get_results_df('results/allmax_50_min_-1_results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dmon_results = dict()\n",
    "for root, _, files in os.walk('results'):\n",
    "    for f in files:\n",
    "        if f.endswith('.xlsx') and f.startswith('dmon__max'):\n",
    "            df = get_results_df(os.path.join(root, f))\n",
    "            for _, r in df.iterrows():\n",
    "                run = r['run']\n",
    "                file_name = r['file_name']\n",
    "                dmon_results[(file_name, run)] = r['metrics']['dmon']\n",
    "\n",
    "def set_dmon_results(df: pd.DataFrame):\n",
    "    for i, r in df.iterrows():\n",
    "        file_name = r['file_name']\n",
    "        run = r['run']\n",
    "        if (file_name, run) in dmon_results:\n",
    "            df.at[i, 'metrics']['dmon'] = dmon_results[(file_name, run)]\n",
    "\n",
    "set_dmon_results(small_models)\n",
    "set_dmon_results(medium_models)\n",
    "set_dmon_results(large_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_updated_df(df):\n",
    "\n",
    "    large_models_results = defaultdict(list)\n",
    "    for _, r in df.iterrows():\n",
    "        file_name = r['file_name']\n",
    "        metrics = r['metrics']\n",
    "        large_models_results[file_name].append(metrics)\n",
    "\n",
    "    updated_result_rows = list()\n",
    "    for file_name, result in large_models_results.items():\n",
    "        gnn_models_results = defaultdict(list)\n",
    "        for r in result:\n",
    "            for k, v in r.items():\n",
    "                gnn_models_results[k].append(v)\n",
    "\n",
    "        for k, v in gnn_models_results.items():\n",
    "            gnn_models_results[k] = max(v, key=lambda x: x['max_cohesion']['cohesion'] * (1 - x['max_cohesion']['coupling']))\n",
    "\n",
    "        updated_result_rows.append({\n",
    "            'file_name': file_name,\n",
    "            'metrics': gnn_models_results\n",
    "        })\n",
    "\n",
    "    updated_df = pd.DataFrame(updated_result_rows) \n",
    "    return updated_df\n",
    "\n",
    "large_models_updated = get_updated_df(large_models)\n",
    "medium_models_updated = get_updated_df(medium_models)\n",
    "small_models_updated = get_updated_df(small_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined results length: 25\n",
      "Combined results length: 104\n",
      "Combined results length: 136\n"
     ]
    }
   ],
   "source": [
    "all_results = get_combined_results_dict(large_models_updated, ga_f2metrics)\n",
    "all_results.update(get_combined_results_dict(medium_models_updated, ga_f2metrics))\n",
    "all_results.update(get_combined_results_dict(small_models_updated, ga_f2metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = get_all_results_metrics(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dgi', 'gnn', 'dmon', 'ga'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results['Buggaboo_j2sw_org.j2sw.parent_org.j2sw_model_generated_Dsl'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2data = {r['file_name']: (r['num_nodes'], r['num_edges']) for _, r in gnn.iterrows()}\n",
    "\n",
    "for i, r in final_result.iterrows():\n",
    "    file_name = r['file_name']\n",
    "    if file_name in f2data:\n",
    "        num_nodes, num_edges = f2data[file_name]\n",
    "        final_result.at[i, 'num_nodes'] = num_nodes\n",
    "        final_result.at[i, 'num_edges'] = num_edges\n",
    "    else:\n",
    "        final_result.at[i, 'num_nodes'] = 0\n",
    "        final_result.at[i, 'num_edges'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_excel('results/final_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 43/136 => 31.6% GNN better than GA on average area for models with nodes <= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_keys = [\n",
    "    'Max Hypervolume Model', \n",
    "    'Max Hypervolume GNN Model', \n",
    "    'Avg. Hypervolume Model',\n",
    "    'Best Hypervolume Model'\n",
    "]\n",
    "cohesion_keys = [\n",
    "    'Max Cohesion Model', \n",
    "    'Max Cohesion (GNNs) Model'\n",
    "]\n",
    "coupling_keys = [\n",
    "    'Min Coupling Model', \n",
    "    'Min Coupling (GNNs) Model'\n",
    "]\n",
    "\n",
    "all_keys = area_keys + cohesion_keys + coupling_keys\n",
    "\n",
    "def get_results_for_key(df, key, min_nodes, max_nodes):\n",
    "    \n",
    "    conf_result = dict()\n",
    "    if key == 'combined' and 'combined' not in df.columns:\n",
    "        df['combined'] = df['num_nodes'] + df['num_edges']\n",
    "\n",
    "    t = df.loc[(df[key] > min_nodes) & (df[key] <= max_nodes)]\n",
    "    info = f\"Results for models with {min_nodes} < {key} <= {max_nodes} with {len(t)} out of {len(df)} models\"\n",
    "    conf_result['info'] = info\n",
    "    print(info)\n",
    "    for key in all_keys:\n",
    "        print(f\"Results for {key}\")\n",
    "        d = dict(t[key].value_counts())\n",
    "        result = {k: f\"{v / sum(d.values())*100:.3f}%\" for k, v in d.items()}\n",
    "        if 'ga' in result:\n",
    "            result['Non GA'] = f\"{(sum(d.values()) - d['ga']) / sum(d.values())*100:.3f}%\"\n",
    "        print(result)\n",
    "        conf_result[key] = result\n",
    "    \n",
    "    print('---')\n",
    "    return conf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = ['num_nodes', 'num_edges', 'combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Small': (0, 163),\n",
       " 'Medium': (163, 424),\n",
       " 'Large': (424, 1778),\n",
       " 'Overall': (0, 1778)}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_ranges(df, key):\n",
    "    data = df[key] if key != 'combined' else df['num_nodes'] + df['num_edges']\n",
    "    p_50 = int(np.percentile(data, 50))\n",
    "    p_90 = int(np.percentile(data, 90))\n",
    "    r1 = (0, p_50)\n",
    "    r2 = (p_50, p_90)\n",
    "    r3 = (p_90, int(max(data)))\n",
    "    d = {\n",
    "        'Small': r1,\n",
    "        'Medium': r2,\n",
    "        'Large': r3,\n",
    "        'Overall': (0, int(max(data)))\n",
    "    }\n",
    "    return d\n",
    "\n",
    "get_ranges(final_result, 'combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for num_nodes\n",
      "Results for Small sized models\n",
      "Results for models with 0 < num_nodes <= 49 with 134 out of 263 models\n",
      "Results for Max Hypervolume Model\n",
      "{'ga': '88.806%', 'gnn': '7.463%', 'dgi': '2.239%', 'dmon': '1.493%', 'Non GA': '11.194%'}\n",
      "Results for Max Hypervolume GNN Model\n",
      "{'gnn': '61.940%', 'dmon': '23.881%', 'dgi': '14.179%'}\n",
      "Results for Avg. Hypervolume Model\n",
      "{'ga': '65.672%', 'gnn': '20.149%', 'dmon': '8.955%', 'dgi': '5.224%', 'Non GA': '34.328%'}\n",
      "Results for Best Hypervolume Model\n",
      "{'ga': '62.687%', 'gnn': '26.866%', 'dmon': '5.970%', 'dgi': '4.478%', 'Non GA': '37.313%'}\n",
      "Results for Max Cohesion Model\n",
      "{'ga': '67.164%', 'gnn': '17.164%', 'dgi': '12.687%', 'dmon': '2.985%', 'Non GA': '32.836%'}\n",
      "Results for Max Cohesion (GNNs) Model\n",
      "{'gnn': '50.000%', 'dgi': '32.836%', 'dmon': '17.164%'}\n",
      "Results for Min Coupling Model\n",
      "{'ga': '40.299%', 'gnn': '30.597%', 'dmon': '26.866%', 'dgi': '2.239%', 'Non GA': '59.701%'}\n",
      "Results for Min Coupling (GNNs) Model\n",
      "{'gnn': '50.746%', 'dmon': '41.791%', 'dgi': '7.463%'}\n",
      "---\n",
      "Results for Medium sized models\n",
      "Results for models with 49 < num_nodes <= 97 with 102 out of 263 models\n",
      "Results for Max Hypervolume Model\n",
      "{'ga': '98.039%', 'gnn': '1.961%', 'Non GA': '1.961%'}\n",
      "Results for Max Hypervolume GNN Model\n",
      "{'gnn': '89.216%', 'dmon': '5.882%', 'dgi': '4.902%'}\n",
      "Results for Avg. Hypervolume Model\n",
      "{'ga': '83.333%', 'gnn': '16.667%', 'Non GA': '16.667%'}\n",
      "Results for Best Hypervolume Model\n",
      "{'ga': '78.431%', 'gnn': '21.569%', 'Non GA': '21.569%'}\n",
      "Results for Max Cohesion Model\n",
      "{'ga': '96.078%', 'gnn': '3.922%', 'Non GA': '3.922%'}\n",
      "Results for Max Cohesion (GNNs) Model\n",
      "{'gnn': '83.333%', 'dgi': '10.784%', 'dmon': '5.882%'}\n",
      "Results for Min Coupling Model\n",
      "{'ga': '44.118%', 'gnn': '41.176%', 'dmon': '11.765%', 'dgi': '2.941%', 'Non GA': '55.882%'}\n",
      "Results for Min Coupling (GNNs) Model\n",
      "{'gnn': '73.529%', 'dmon': '20.588%', 'dgi': '5.882%'}\n",
      "---\n",
      "Results for Large sized models\n",
      "Results for models with 97 < num_nodes <= 226 with 27 out of 263 models\n",
      "Results for Max Hypervolume Model\n",
      "{'ga': '92.593%', 'gnn': '7.407%', 'Non GA': '7.407%'}\n",
      "Results for Max Hypervolume GNN Model\n",
      "{'gnn': '100.000%'}\n",
      "Results for Avg. Hypervolume Model\n",
      "{'ga': '66.667%', 'gnn': '33.333%', 'Non GA': '33.333%'}\n",
      "Results for Best Hypervolume Model\n",
      "{'ga': '66.667%', 'gnn': '33.333%', 'Non GA': '33.333%'}\n",
      "Results for Max Cohesion Model\n",
      "{'ga': '85.185%', 'gnn': '11.111%', 'dgi': '3.704%', 'Non GA': '14.815%'}\n",
      "Results for Max Cohesion (GNNs) Model\n",
      "{'gnn': '81.481%', 'dgi': '11.111%', 'dmon': '7.407%'}\n",
      "Results for Min Coupling Model\n",
      "{'gnn': '62.963%', 'ga': '29.630%', 'dmon': '3.704%', 'dgi': '3.704%', 'Non GA': '70.370%'}\n",
      "Results for Min Coupling (GNNs) Model\n",
      "{'gnn': '81.481%', 'dmon': '11.111%', 'dgi': '7.407%'}\n",
      "---\n",
      "Results for Overall sized models\n",
      "Results for models with 0 < num_nodes <= 226 with 263 out of 263 models\n",
      "Results for Max Hypervolume Model\n",
      "{'ga': '92.776%', 'gnn': '5.323%', 'dgi': '1.141%', 'dmon': '0.760%', 'Non GA': '7.224%'}\n",
      "Results for Max Hypervolume GNN Model\n",
      "{'gnn': '76.426%', 'dmon': '14.449%', 'dgi': '9.125%'}\n",
      "Results for Avg. Hypervolume Model\n",
      "{'ga': '72.624%', 'gnn': '20.152%', 'dmon': '4.563%', 'dgi': '2.662%', 'Non GA': '27.376%'}\n",
      "Results for Best Hypervolume Model\n",
      "{'ga': '69.202%', 'gnn': '25.475%', 'dmon': '3.042%', 'dgi': '2.281%', 'Non GA': '30.798%'}\n",
      "Results for Max Cohesion Model\n",
      "{'ga': '80.228%', 'gnn': '11.407%', 'dgi': '6.844%', 'dmon': '1.521%', 'Non GA': '19.772%'}\n",
      "Results for Max Cohesion (GNNs) Model\n",
      "{'gnn': '66.160%', 'dgi': '22.053%', 'dmon': '11.787%'}\n",
      "Results for Min Coupling Model\n",
      "{'ga': '40.684%', 'gnn': '38.023%', 'dmon': '18.631%', 'dgi': '2.662%', 'Non GA': '59.316%'}\n",
      "Results for Min Coupling (GNNs) Model\n",
      "{'gnn': '62.738%', 'dmon': '30.418%', 'dgi': '6.844%'}\n",
      "---\n",
      "---\n",
      "\n",
      "Results for num_edges\n",
      "Results for Small sized models\n",
      "Results for models with 0 < num_edges <= 113 with 136 out of 263 models\n",
      "Results for Max Hypervolume Model\n",
      "{'ga': '88.235%', 'gnn': '8.088%', 'dgi': '2.206%', 'dmon': '1.471%', 'Non GA': '11.765%'}\n",
      "Results for Max Hypervolume GNN Model\n",
      "{'gnn': '70.588%', 'dmon': '16.176%', 'dgi': '13.235%'}\n",
      "Results for Avg. Hypervolume Model\n",
      "{'ga': '66.176%', 'gnn': '22.059%', 'dmon': '6.618%', 'dgi': '5.147%', 'Non GA': '33.824%'}\n",
      "Results for Best Hypervolume Model\n",
      "{'ga': '63.235%', 'gnn': '27.941%', 'dgi': '4.412%', 'dmon': '4.412%', 'Non GA': '36.765%'}\n",
      "Results for Max Cohesion Model\n",
      "{'ga': '72.794%', 'gnn': '15.441%', 'dgi': '10.294%', 'dmon': '1.471%', 'Non GA': '27.206%'}\n",
      "Results for Max Cohesion (GNNs) Model\n",
      "{'gnn': '60.294%', 'dgi': '25.735%', 'dmon': '13.971%'}\n",
      "Results for Min Coupling Model\n",
      "{'ga': '34.559%', 'gnn': '33.824%', 'dmon': '27.941%', 'dgi': '3.676%', 'Non GA': '65.441%'}\n",
      "Results for Min Coupling (GNNs) Model\n",
      "{'gnn': '50.000%', 'dmon': '40.441%', 'dgi': '9.559%'}\n",
      "---\n",
      "Results for Medium sized models\n",
      "Results for models with 113 < num_edges <= 303 with 100 out of 263 models\n",
      "Results for Max Hypervolume Model\n",
      "{'ga': '99.000%', 'gnn': '1.000%', 'Non GA': '1.000%'}\n",
      "Results for Max Hypervolume GNN Model\n",
      "{'gnn': '80.000%', 'dmon': '15.000%', 'dgi': '5.000%'}\n",
      "Results for Avg. Hypervolume Model\n",
      "{'ga': '84.000%', 'gnn': '13.000%', 'dmon': '3.000%', 'Non GA': '16.000%'}\n",
      "Results for Best Hypervolume Model\n",
      "{'ga': '81.000%', 'gnn': '17.000%', 'dmon': '2.000%', 'Non GA': '19.000%'}\n",
      "Results for Max Cohesion Model\n",
      "{'ga': '89.000%', 'gnn': '6.000%', 'dgi': '3.000%', 'dmon': '2.000%', 'Non GA': '11.000%'}\n",
      "Results for Max Cohesion (GNNs) Model\n",
      "{'gnn': '72.000%', 'dgi': '19.000%', 'dmon': '9.000%'}\n",
      "Results for Min Coupling Model\n",
      "{'ga': '46.000%', 'gnn': '42.000%', 'dmon': '11.000%', 'dgi': '1.000%', 'Non GA': '54.000%'}\n",
      "Results for Min Coupling (GNNs) Model\n",
      "{'gnn': '75.000%', 'dmon': '22.000%', 'dgi': '3.000%'}\n",
      "---\n",
      "Results for Large sized models\n",
      "Results for models with 303 < num_edges <= 1552 with 27 out of 263 models\n",
      "Results for Max Hypervolume Model\n",
      "{'ga': '92.593%', 'gnn': '7.407%', 'Non GA': '7.407%'}\n",
      "Results for Max Hypervolume GNN Model\n",
      "{'gnn': '92.593%', 'dmon': '3.704%', 'dgi': '3.704%'}\n",
      "Results for Avg. Hypervolume Model\n",
      "{'ga': '62.963%', 'gnn': '37.037%', 'Non GA': '37.037%'}\n",
      "Results for Best Hypervolume Model\n",
      "{'ga': '55.556%', 'gnn': '44.444%', 'Non GA': '44.444%'}\n",
      "Results for Max Cohesion Model\n",
      "{'ga': '85.185%', 'gnn': '11.111%', 'dgi': '3.704%', 'Non GA': '14.815%'}\n",
      "Results for Max Cohesion (GNNs) Model\n",
      "{'gnn': '74.074%', 'dgi': '14.815%', 'dmon': '11.111%'}\n",
      "Results for Min Coupling Model\n",
      "{'ga': '51.852%', 'gnn': '44.444%', 'dgi': '3.704%', 'Non GA': '48.148%'}\n",
      "Results for Min Coupling (GNNs) Model\n",
      "{'gnn': '81.481%', 'dmon': '11.111%', 'dgi': '7.407%'}\n",
      "---\n",
      "Results for Overall sized models\n",
      "Results for models with 0 < num_edges <= 1552 with 263 out of 263 models\n",
      "Results for Max Hypervolume Model\n",
      "{'ga': '92.776%', 'gnn': '5.323%', 'dgi': '1.141%', 'dmon': '0.760%', 'Non GA': '7.224%'}\n",
      "Results for Max Hypervolume GNN Model\n",
      "{'gnn': '76.426%', 'dmon': '14.449%', 'dgi': '9.125%'}\n",
      "Results for Avg. Hypervolume Model\n",
      "{'ga': '72.624%', 'gnn': '20.152%', 'dmon': '4.563%', 'dgi': '2.662%', 'Non GA': '27.376%'}\n",
      "Results for Best Hypervolume Model\n",
      "{'ga': '69.202%', 'gnn': '25.475%', 'dmon': '3.042%', 'dgi': '2.281%', 'Non GA': '30.798%'}\n",
      "Results for Max Cohesion Model\n",
      "{'ga': '80.228%', 'gnn': '11.407%', 'dgi': '6.844%', 'dmon': '1.521%', 'Non GA': '19.772%'}\n",
      "Results for Max Cohesion (GNNs) Model\n",
      "{'gnn': '66.160%', 'dgi': '22.053%', 'dmon': '11.787%'}\n",
      "Results for Min Coupling Model\n",
      "{'ga': '40.684%', 'gnn': '38.023%', 'dmon': '18.631%', 'dgi': '2.662%', 'Non GA': '59.316%'}\n",
      "Results for Min Coupling (GNNs) Model\n",
      "{'gnn': '62.738%', 'dmon': '30.418%', 'dgi': '6.844%'}\n",
      "---\n",
      "---\n",
      "\n",
      "Results for combined\n",
      "Results for Small sized models\n",
      "Results for models with 0 < combined <= 163 with 132 out of 263 models\n",
      "Results for Max Hypervolume Model\n",
      "{'ga': '87.879%', 'gnn': '8.333%', 'dgi': '2.273%', 'dmon': '1.515%', 'Non GA': '12.121%'}\n",
      "Results for Max Hypervolume GNN Model\n",
      "{'gnn': '66.667%', 'dmon': '19.697%', 'dgi': '13.636%'}\n",
      "Results for Avg. Hypervolume Model\n",
      "{'ga': '66.667%', 'gnn': '21.212%', 'dmon': '6.818%', 'dgi': '5.303%', 'Non GA': '33.333%'}\n",
      "Results for Best Hypervolume Model\n",
      "{'ga': '63.636%', 'gnn': '27.273%', 'dgi': '4.545%', 'dmon': '4.545%', 'Non GA': '36.364%'}\n",
      "Results for Max Cohesion Model\n",
      "{'ga': '68.182%', 'gnn': '17.424%', 'dgi': '11.364%', 'dmon': '3.030%', 'Non GA': '31.818%'}\n",
      "Results for Max Cohesion (GNNs) Model\n",
      "{'gnn': '55.303%', 'dgi': '28.788%', 'dmon': '15.909%'}\n",
      "Results for Min Coupling Model\n",
      "{'ga': '36.364%', 'gnn': '31.818%', 'dmon': '28.030%', 'dgi': '3.788%', 'Non GA': '63.636%'}\n",
      "Results for Min Coupling (GNNs) Model\n",
      "{'gnn': '47.727%', 'dmon': '42.424%', 'dgi': '9.848%'}\n",
      "---\n",
      "Results for Medium sized models\n",
      "Results for models with 163 < combined <= 424 with 104 out of 263 models\n",
      "Results for Max Hypervolume Model\n",
      "{'ga': '99.038%', 'gnn': '0.962%', 'Non GA': '0.962%'}\n",
      "Results for Max Hypervolume GNN Model\n",
      "{'gnn': '84.615%', 'dmon': '10.577%', 'dgi': '4.808%'}\n",
      "Results for Avg. Hypervolume Model\n",
      "{'ga': '82.692%', 'gnn': '14.423%', 'dmon': '2.885%', 'Non GA': '17.308%'}\n",
      "Results for Best Hypervolume Model\n",
      "{'ga': '79.808%', 'gnn': '18.269%', 'dmon': '1.923%', 'Non GA': '20.192%'}\n",
      "Results for Max Cohesion Model\n",
      "{'ga': '94.231%', 'gnn': '3.846%', 'dgi': '1.923%', 'Non GA': '5.769%'}\n",
      "Results for Max Cohesion (GNNs) Model\n",
      "{'gnn': '77.885%', 'dgi': '15.385%', 'dmon': '6.731%'}\n",
      "Results for Min Coupling Model\n",
      "{'gnn': '44.231%', 'ga': '44.231%', 'dmon': '10.577%', 'dgi': '0.962%', 'Non GA': '55.769%'}\n",
      "Results for Min Coupling (GNNs) Model\n",
      "{'gnn': '77.885%', 'dmon': '19.231%', 'dgi': '2.885%'}\n",
      "---\n",
      "Results for Large sized models\n",
      "Results for models with 424 < combined <= 1778 with 27 out of 263 models\n",
      "Results for Max Hypervolume Model\n",
      "{'ga': '92.593%', 'gnn': '7.407%', 'Non GA': '7.407%'}\n",
      "Results for Max Hypervolume GNN Model\n",
      "{'gnn': '92.593%', 'dmon': '3.704%', 'dgi': '3.704%'}\n",
      "Results for Avg. Hypervolume Model\n",
      "{'ga': '62.963%', 'gnn': '37.037%', 'Non GA': '37.037%'}\n",
      "Results for Best Hypervolume Model\n",
      "{'ga': '55.556%', 'gnn': '44.444%', 'Non GA': '44.444%'}\n",
      "Results for Max Cohesion Model\n",
      "{'ga': '85.185%', 'gnn': '11.111%', 'dgi': '3.704%', 'Non GA': '14.815%'}\n",
      "Results for Max Cohesion (GNNs) Model\n",
      "{'gnn': '74.074%', 'dgi': '14.815%', 'dmon': '11.111%'}\n",
      "Results for Min Coupling Model\n",
      "{'ga': '48.148%', 'gnn': '44.444%', 'dmon': '3.704%', 'dgi': '3.704%', 'Non GA': '51.852%'}\n",
      "Results for Min Coupling (GNNs) Model\n",
      "{'gnn': '77.778%', 'dmon': '14.815%', 'dgi': '7.407%'}\n",
      "---\n",
      "Results for Overall sized models\n",
      "Results for models with 0 < combined <= 1778 with 263 out of 263 models\n",
      "Results for Max Hypervolume Model\n",
      "{'ga': '92.776%', 'gnn': '5.323%', 'dgi': '1.141%', 'dmon': '0.760%', 'Non GA': '7.224%'}\n",
      "Results for Max Hypervolume GNN Model\n",
      "{'gnn': '76.426%', 'dmon': '14.449%', 'dgi': '9.125%'}\n",
      "Results for Avg. Hypervolume Model\n",
      "{'ga': '72.624%', 'gnn': '20.152%', 'dmon': '4.563%', 'dgi': '2.662%', 'Non GA': '27.376%'}\n",
      "Results for Best Hypervolume Model\n",
      "{'ga': '69.202%', 'gnn': '25.475%', 'dmon': '3.042%', 'dgi': '2.281%', 'Non GA': '30.798%'}\n",
      "Results for Max Cohesion Model\n",
      "{'ga': '80.228%', 'gnn': '11.407%', 'dgi': '6.844%', 'dmon': '1.521%', 'Non GA': '19.772%'}\n",
      "Results for Max Cohesion (GNNs) Model\n",
      "{'gnn': '66.160%', 'dgi': '22.053%', 'dmon': '11.787%'}\n",
      "Results for Min Coupling Model\n",
      "{'ga': '40.684%', 'gnn': '38.023%', 'dmon': '18.631%', 'dgi': '2.662%', 'Non GA': '59.316%'}\n",
      "Results for Min Coupling (GNNs) Model\n",
      "{'gnn': '62.738%', 'dmon': '30.418%', 'dgi': '6.844%'}\n",
      "---\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_results = {config_key: {k: dict() for k in get_ranges(final_result, config_key).keys()} for config_key in configs}\n",
    "\n",
    "for config_key in configs:\n",
    "    print(f\"Results for {config_key}\")\n",
    "    ranges = get_ranges(final_result, config_key)\n",
    "    for k, v in ranges.items():\n",
    "        print(f\"Results for {k} sized models\")\n",
    "        config_result = get_results_for_key(final_result, config_key, v[0], v[1])\n",
    "        config_results[config_key][k] = config_result\n",
    "    \n",
    "    print('---\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Results for Max Hypervolume Model\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_nodes\n",
      "Results for Small\n",
      "Results for models with 0 < num_nodes <= 49 with 134 out of 263 models\n",
      "{'ga': '88.806%', 'gnn': '7.463%', 'dgi': '2.239%', 'dmon': '1.493%', 'Non GA': '11.194%'}\n",
      "Results for Medium\n",
      "Results for models with 49 < num_nodes <= 97 with 102 out of 263 models\n",
      "{'ga': '98.039%', 'gnn': '1.961%', 'Non GA': '1.961%'}\n",
      "Results for Large\n",
      "Results for models with 97 < num_nodes <= 226 with 27 out of 263 models\n",
      "{'ga': '92.593%', 'gnn': '7.407%', 'Non GA': '7.407%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_nodes <= 226 with 263 out of 263 models\n",
      "{'ga': '92.776%', 'gnn': '5.323%', 'dgi': '1.141%', 'dmon': '0.760%', 'Non GA': '7.224%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_edges\n",
      "Results for Small\n",
      "Results for models with 0 < num_edges <= 113 with 136 out of 263 models\n",
      "{'ga': '88.235%', 'gnn': '8.088%', 'dgi': '2.206%', 'dmon': '1.471%', 'Non GA': '11.765%'}\n",
      "Results for Medium\n",
      "Results for models with 113 < num_edges <= 303 with 100 out of 263 models\n",
      "{'ga': '99.000%', 'gnn': '1.000%', 'Non GA': '1.000%'}\n",
      "Results for Large\n",
      "Results for models with 303 < num_edges <= 1552 with 27 out of 263 models\n",
      "{'ga': '92.593%', 'gnn': '7.407%', 'Non GA': '7.407%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_edges <= 1552 with 263 out of 263 models\n",
      "{'ga': '92.776%', 'gnn': '5.323%', 'dgi': '1.141%', 'dmon': '0.760%', 'Non GA': '7.224%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: combined\n",
      "Results for Small\n",
      "Results for models with 0 < combined <= 163 with 132 out of 263 models\n",
      "{'ga': '87.879%', 'gnn': '8.333%', 'dgi': '2.273%', 'dmon': '1.515%', 'Non GA': '12.121%'}\n",
      "Results for Medium\n",
      "Results for models with 163 < combined <= 424 with 104 out of 263 models\n",
      "{'ga': '99.038%', 'gnn': '0.962%', 'Non GA': '0.962%'}\n",
      "Results for Large\n",
      "Results for models with 424 < combined <= 1778 with 27 out of 263 models\n",
      "{'ga': '92.593%', 'gnn': '7.407%', 'Non GA': '7.407%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < combined <= 1778 with 263 out of 263 models\n",
      "{'ga': '92.776%', 'gnn': '5.323%', 'dgi': '1.141%', 'dmon': '0.760%', 'Non GA': '7.224%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for Max Hypervolume GNN Model\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_nodes\n",
      "Results for Small\n",
      "Results for models with 0 < num_nodes <= 49 with 134 out of 263 models\n",
      "{'gnn': '61.940%', 'dmon': '23.881%', 'dgi': '14.179%'}\n",
      "Results for Medium\n",
      "Results for models with 49 < num_nodes <= 97 with 102 out of 263 models\n",
      "{'gnn': '89.216%', 'dmon': '5.882%', 'dgi': '4.902%'}\n",
      "Results for Large\n",
      "Results for models with 97 < num_nodes <= 226 with 27 out of 263 models\n",
      "{'gnn': '100.000%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_nodes <= 226 with 263 out of 263 models\n",
      "{'gnn': '76.426%', 'dmon': '14.449%', 'dgi': '9.125%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_edges\n",
      "Results for Small\n",
      "Results for models with 0 < num_edges <= 113 with 136 out of 263 models\n",
      "{'gnn': '70.588%', 'dmon': '16.176%', 'dgi': '13.235%'}\n",
      "Results for Medium\n",
      "Results for models with 113 < num_edges <= 303 with 100 out of 263 models\n",
      "{'gnn': '80.000%', 'dmon': '15.000%', 'dgi': '5.000%'}\n",
      "Results for Large\n",
      "Results for models with 303 < num_edges <= 1552 with 27 out of 263 models\n",
      "{'gnn': '92.593%', 'dmon': '3.704%', 'dgi': '3.704%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_edges <= 1552 with 263 out of 263 models\n",
      "{'gnn': '76.426%', 'dmon': '14.449%', 'dgi': '9.125%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: combined\n",
      "Results for Small\n",
      "Results for models with 0 < combined <= 163 with 132 out of 263 models\n",
      "{'gnn': '66.667%', 'dmon': '19.697%', 'dgi': '13.636%'}\n",
      "Results for Medium\n",
      "Results for models with 163 < combined <= 424 with 104 out of 263 models\n",
      "{'gnn': '84.615%', 'dmon': '10.577%', 'dgi': '4.808%'}\n",
      "Results for Large\n",
      "Results for models with 424 < combined <= 1778 with 27 out of 263 models\n",
      "{'gnn': '92.593%', 'dmon': '3.704%', 'dgi': '3.704%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < combined <= 1778 with 263 out of 263 models\n",
      "{'gnn': '76.426%', 'dmon': '14.449%', 'dgi': '9.125%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for Avg. Hypervolume Model\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_nodes\n",
      "Results for Small\n",
      "Results for models with 0 < num_nodes <= 49 with 134 out of 263 models\n",
      "{'ga': '65.672%', 'gnn': '20.149%', 'dmon': '8.955%', 'dgi': '5.224%', 'Non GA': '34.328%'}\n",
      "Results for Medium\n",
      "Results for models with 49 < num_nodes <= 97 with 102 out of 263 models\n",
      "{'ga': '83.333%', 'gnn': '16.667%', 'Non GA': '16.667%'}\n",
      "Results for Large\n",
      "Results for models with 97 < num_nodes <= 226 with 27 out of 263 models\n",
      "{'ga': '66.667%', 'gnn': '33.333%', 'Non GA': '33.333%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_nodes <= 226 with 263 out of 263 models\n",
      "{'ga': '72.624%', 'gnn': '20.152%', 'dmon': '4.563%', 'dgi': '2.662%', 'Non GA': '27.376%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_edges\n",
      "Results for Small\n",
      "Results for models with 0 < num_edges <= 113 with 136 out of 263 models\n",
      "{'ga': '66.176%', 'gnn': '22.059%', 'dmon': '6.618%', 'dgi': '5.147%', 'Non GA': '33.824%'}\n",
      "Results for Medium\n",
      "Results for models with 113 < num_edges <= 303 with 100 out of 263 models\n",
      "{'ga': '84.000%', 'gnn': '13.000%', 'dmon': '3.000%', 'Non GA': '16.000%'}\n",
      "Results for Large\n",
      "Results for models with 303 < num_edges <= 1552 with 27 out of 263 models\n",
      "{'ga': '62.963%', 'gnn': '37.037%', 'Non GA': '37.037%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_edges <= 1552 with 263 out of 263 models\n",
      "{'ga': '72.624%', 'gnn': '20.152%', 'dmon': '4.563%', 'dgi': '2.662%', 'Non GA': '27.376%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: combined\n",
      "Results for Small\n",
      "Results for models with 0 < combined <= 163 with 132 out of 263 models\n",
      "{'ga': '66.667%', 'gnn': '21.212%', 'dmon': '6.818%', 'dgi': '5.303%', 'Non GA': '33.333%'}\n",
      "Results for Medium\n",
      "Results for models with 163 < combined <= 424 with 104 out of 263 models\n",
      "{'ga': '82.692%', 'gnn': '14.423%', 'dmon': '2.885%', 'Non GA': '17.308%'}\n",
      "Results for Large\n",
      "Results for models with 424 < combined <= 1778 with 27 out of 263 models\n",
      "{'ga': '62.963%', 'gnn': '37.037%', 'Non GA': '37.037%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < combined <= 1778 with 263 out of 263 models\n",
      "{'ga': '72.624%', 'gnn': '20.152%', 'dmon': '4.563%', 'dgi': '2.662%', 'Non GA': '27.376%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for Best Hypervolume Model\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_nodes\n",
      "Results for Small\n",
      "Results for models with 0 < num_nodes <= 49 with 134 out of 263 models\n",
      "{'ga': '62.687%', 'gnn': '26.866%', 'dmon': '5.970%', 'dgi': '4.478%', 'Non GA': '37.313%'}\n",
      "Results for Medium\n",
      "Results for models with 49 < num_nodes <= 97 with 102 out of 263 models\n",
      "{'ga': '78.431%', 'gnn': '21.569%', 'Non GA': '21.569%'}\n",
      "Results for Large\n",
      "Results for models with 97 < num_nodes <= 226 with 27 out of 263 models\n",
      "{'ga': '66.667%', 'gnn': '33.333%', 'Non GA': '33.333%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_nodes <= 226 with 263 out of 263 models\n",
      "{'ga': '69.202%', 'gnn': '25.475%', 'dmon': '3.042%', 'dgi': '2.281%', 'Non GA': '30.798%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_edges\n",
      "Results for Small\n",
      "Results for models with 0 < num_edges <= 113 with 136 out of 263 models\n",
      "{'ga': '63.235%', 'gnn': '27.941%', 'dgi': '4.412%', 'dmon': '4.412%', 'Non GA': '36.765%'}\n",
      "Results for Medium\n",
      "Results for models with 113 < num_edges <= 303 with 100 out of 263 models\n",
      "{'ga': '81.000%', 'gnn': '17.000%', 'dmon': '2.000%', 'Non GA': '19.000%'}\n",
      "Results for Large\n",
      "Results for models with 303 < num_edges <= 1552 with 27 out of 263 models\n",
      "{'ga': '55.556%', 'gnn': '44.444%', 'Non GA': '44.444%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_edges <= 1552 with 263 out of 263 models\n",
      "{'ga': '69.202%', 'gnn': '25.475%', 'dmon': '3.042%', 'dgi': '2.281%', 'Non GA': '30.798%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: combined\n",
      "Results for Small\n",
      "Results for models with 0 < combined <= 163 with 132 out of 263 models\n",
      "{'ga': '63.636%', 'gnn': '27.273%', 'dgi': '4.545%', 'dmon': '4.545%', 'Non GA': '36.364%'}\n",
      "Results for Medium\n",
      "Results for models with 163 < combined <= 424 with 104 out of 263 models\n",
      "{'ga': '79.808%', 'gnn': '18.269%', 'dmon': '1.923%', 'Non GA': '20.192%'}\n",
      "Results for Large\n",
      "Results for models with 424 < combined <= 1778 with 27 out of 263 models\n",
      "{'ga': '55.556%', 'gnn': '44.444%', 'Non GA': '44.444%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < combined <= 1778 with 263 out of 263 models\n",
      "{'ga': '69.202%', 'gnn': '25.475%', 'dmon': '3.042%', 'dgi': '2.281%', 'Non GA': '30.798%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for Max Cohesion Model\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_nodes\n",
      "Results for Small\n",
      "Results for models with 0 < num_nodes <= 49 with 134 out of 263 models\n",
      "{'ga': '67.164%', 'gnn': '17.164%', 'dgi': '12.687%', 'dmon': '2.985%', 'Non GA': '32.836%'}\n",
      "Results for Medium\n",
      "Results for models with 49 < num_nodes <= 97 with 102 out of 263 models\n",
      "{'ga': '96.078%', 'gnn': '3.922%', 'Non GA': '3.922%'}\n",
      "Results for Large\n",
      "Results for models with 97 < num_nodes <= 226 with 27 out of 263 models\n",
      "{'ga': '85.185%', 'gnn': '11.111%', 'dgi': '3.704%', 'Non GA': '14.815%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_nodes <= 226 with 263 out of 263 models\n",
      "{'ga': '80.228%', 'gnn': '11.407%', 'dgi': '6.844%', 'dmon': '1.521%', 'Non GA': '19.772%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_edges\n",
      "Results for Small\n",
      "Results for models with 0 < num_edges <= 113 with 136 out of 263 models\n",
      "{'ga': '72.794%', 'gnn': '15.441%', 'dgi': '10.294%', 'dmon': '1.471%', 'Non GA': '27.206%'}\n",
      "Results for Medium\n",
      "Results for models with 113 < num_edges <= 303 with 100 out of 263 models\n",
      "{'ga': '89.000%', 'gnn': '6.000%', 'dgi': '3.000%', 'dmon': '2.000%', 'Non GA': '11.000%'}\n",
      "Results for Large\n",
      "Results for models with 303 < num_edges <= 1552 with 27 out of 263 models\n",
      "{'ga': '85.185%', 'gnn': '11.111%', 'dgi': '3.704%', 'Non GA': '14.815%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_edges <= 1552 with 263 out of 263 models\n",
      "{'ga': '80.228%', 'gnn': '11.407%', 'dgi': '6.844%', 'dmon': '1.521%', 'Non GA': '19.772%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: combined\n",
      "Results for Small\n",
      "Results for models with 0 < combined <= 163 with 132 out of 263 models\n",
      "{'ga': '68.182%', 'gnn': '17.424%', 'dgi': '11.364%', 'dmon': '3.030%', 'Non GA': '31.818%'}\n",
      "Results for Medium\n",
      "Results for models with 163 < combined <= 424 with 104 out of 263 models\n",
      "{'ga': '94.231%', 'gnn': '3.846%', 'dgi': '1.923%', 'Non GA': '5.769%'}\n",
      "Results for Large\n",
      "Results for models with 424 < combined <= 1778 with 27 out of 263 models\n",
      "{'ga': '85.185%', 'gnn': '11.111%', 'dgi': '3.704%', 'Non GA': '14.815%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < combined <= 1778 with 263 out of 263 models\n",
      "{'ga': '80.228%', 'gnn': '11.407%', 'dgi': '6.844%', 'dmon': '1.521%', 'Non GA': '19.772%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for Max Cohesion (GNNs) Model\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_nodes\n",
      "Results for Small\n",
      "Results for models with 0 < num_nodes <= 49 with 134 out of 263 models\n",
      "{'gnn': '50.000%', 'dgi': '32.836%', 'dmon': '17.164%'}\n",
      "Results for Medium\n",
      "Results for models with 49 < num_nodes <= 97 with 102 out of 263 models\n",
      "{'gnn': '83.333%', 'dgi': '10.784%', 'dmon': '5.882%'}\n",
      "Results for Large\n",
      "Results for models with 97 < num_nodes <= 226 with 27 out of 263 models\n",
      "{'gnn': '81.481%', 'dgi': '11.111%', 'dmon': '7.407%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_nodes <= 226 with 263 out of 263 models\n",
      "{'gnn': '66.160%', 'dgi': '22.053%', 'dmon': '11.787%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_edges\n",
      "Results for Small\n",
      "Results for models with 0 < num_edges <= 113 with 136 out of 263 models\n",
      "{'gnn': '60.294%', 'dgi': '25.735%', 'dmon': '13.971%'}\n",
      "Results for Medium\n",
      "Results for models with 113 < num_edges <= 303 with 100 out of 263 models\n",
      "{'gnn': '72.000%', 'dgi': '19.000%', 'dmon': '9.000%'}\n",
      "Results for Large\n",
      "Results for models with 303 < num_edges <= 1552 with 27 out of 263 models\n",
      "{'gnn': '74.074%', 'dgi': '14.815%', 'dmon': '11.111%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_edges <= 1552 with 263 out of 263 models\n",
      "{'gnn': '66.160%', 'dgi': '22.053%', 'dmon': '11.787%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: combined\n",
      "Results for Small\n",
      "Results for models with 0 < combined <= 163 with 132 out of 263 models\n",
      "{'gnn': '55.303%', 'dgi': '28.788%', 'dmon': '15.909%'}\n",
      "Results for Medium\n",
      "Results for models with 163 < combined <= 424 with 104 out of 263 models\n",
      "{'gnn': '77.885%', 'dgi': '15.385%', 'dmon': '6.731%'}\n",
      "Results for Large\n",
      "Results for models with 424 < combined <= 1778 with 27 out of 263 models\n",
      "{'gnn': '74.074%', 'dgi': '14.815%', 'dmon': '11.111%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < combined <= 1778 with 263 out of 263 models\n",
      "{'gnn': '66.160%', 'dgi': '22.053%', 'dmon': '11.787%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for Min Coupling Model\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_nodes\n",
      "Results for Small\n",
      "Results for models with 0 < num_nodes <= 49 with 134 out of 263 models\n",
      "{'ga': '40.299%', 'gnn': '30.597%', 'dmon': '26.866%', 'dgi': '2.239%', 'Non GA': '59.701%'}\n",
      "Results for Medium\n",
      "Results for models with 49 < num_nodes <= 97 with 102 out of 263 models\n",
      "{'ga': '44.118%', 'gnn': '41.176%', 'dmon': '11.765%', 'dgi': '2.941%', 'Non GA': '55.882%'}\n",
      "Results for Large\n",
      "Results for models with 97 < num_nodes <= 226 with 27 out of 263 models\n",
      "{'gnn': '62.963%', 'ga': '29.630%', 'dmon': '3.704%', 'dgi': '3.704%', 'Non GA': '70.370%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_nodes <= 226 with 263 out of 263 models\n",
      "{'ga': '40.684%', 'gnn': '38.023%', 'dmon': '18.631%', 'dgi': '2.662%', 'Non GA': '59.316%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_edges\n",
      "Results for Small\n",
      "Results for models with 0 < num_edges <= 113 with 136 out of 263 models\n",
      "{'ga': '34.559%', 'gnn': '33.824%', 'dmon': '27.941%', 'dgi': '3.676%', 'Non GA': '65.441%'}\n",
      "Results for Medium\n",
      "Results for models with 113 < num_edges <= 303 with 100 out of 263 models\n",
      "{'ga': '46.000%', 'gnn': '42.000%', 'dmon': '11.000%', 'dgi': '1.000%', 'Non GA': '54.000%'}\n",
      "Results for Large\n",
      "Results for models with 303 < num_edges <= 1552 with 27 out of 263 models\n",
      "{'ga': '51.852%', 'gnn': '44.444%', 'dgi': '3.704%', 'Non GA': '48.148%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_edges <= 1552 with 263 out of 263 models\n",
      "{'ga': '40.684%', 'gnn': '38.023%', 'dmon': '18.631%', 'dgi': '2.662%', 'Non GA': '59.316%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: combined\n",
      "Results for Small\n",
      "Results for models with 0 < combined <= 163 with 132 out of 263 models\n",
      "{'ga': '36.364%', 'gnn': '31.818%', 'dmon': '28.030%', 'dgi': '3.788%', 'Non GA': '63.636%'}\n",
      "Results for Medium\n",
      "Results for models with 163 < combined <= 424 with 104 out of 263 models\n",
      "{'gnn': '44.231%', 'ga': '44.231%', 'dmon': '10.577%', 'dgi': '0.962%', 'Non GA': '55.769%'}\n",
      "Results for Large\n",
      "Results for models with 424 < combined <= 1778 with 27 out of 263 models\n",
      "{'ga': '48.148%', 'gnn': '44.444%', 'dmon': '3.704%', 'dgi': '3.704%', 'Non GA': '51.852%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < combined <= 1778 with 263 out of 263 models\n",
      "{'ga': '40.684%', 'gnn': '38.023%', 'dmon': '18.631%', 'dgi': '2.662%', 'Non GA': '59.316%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for Min Coupling (GNNs) Model\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_nodes\n",
      "Results for Small\n",
      "Results for models with 0 < num_nodes <= 49 with 134 out of 263 models\n",
      "{'gnn': '50.746%', 'dmon': '41.791%', 'dgi': '7.463%'}\n",
      "Results for Medium\n",
      "Results for models with 49 < num_nodes <= 97 with 102 out of 263 models\n",
      "{'gnn': '73.529%', 'dmon': '20.588%', 'dgi': '5.882%'}\n",
      "Results for Large\n",
      "Results for models with 97 < num_nodes <= 226 with 27 out of 263 models\n",
      "{'gnn': '81.481%', 'dmon': '11.111%', 'dgi': '7.407%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_nodes <= 226 with 263 out of 263 models\n",
      "{'gnn': '62.738%', 'dmon': '30.418%', 'dgi': '6.844%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: num_edges\n",
      "Results for Small\n",
      "Results for models with 0 < num_edges <= 113 with 136 out of 263 models\n",
      "{'gnn': '50.000%', 'dmon': '40.441%', 'dgi': '9.559%'}\n",
      "Results for Medium\n",
      "Results for models with 113 < num_edges <= 303 with 100 out of 263 models\n",
      "{'gnn': '75.000%', 'dmon': '22.000%', 'dgi': '3.000%'}\n",
      "Results for Large\n",
      "Results for models with 303 < num_edges <= 1552 with 27 out of 263 models\n",
      "{'gnn': '81.481%', 'dmon': '11.111%', 'dgi': '7.407%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < num_edges <= 1552 with 263 out of 263 models\n",
      "{'gnn': '62.738%', 'dmon': '30.418%', 'dgi': '6.844%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for considering model size as: combined\n",
      "Results for Small\n",
      "Results for models with 0 < combined <= 163 with 132 out of 263 models\n",
      "{'gnn': '47.727%', 'dmon': '42.424%', 'dgi': '9.848%'}\n",
      "Results for Medium\n",
      "Results for models with 163 < combined <= 424 with 104 out of 263 models\n",
      "{'gnn': '77.885%', 'dmon': '19.231%', 'dgi': '2.885%'}\n",
      "Results for Large\n",
      "Results for models with 424 < combined <= 1778 with 27 out of 263 models\n",
      "{'gnn': '77.778%', 'dmon': '14.815%', 'dgi': '7.407%'}\n",
      "Results for Overall\n",
      "Results for models with 0 < combined <= 1778 with 263 out of 263 models\n",
      "{'gnn': '62.738%', 'dmon': '30.418%', 'dgi': '6.844%'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "size_type_keys = list(config_results.keys())\n",
    "size_keys = list(config_results[size_type_keys[0]].keys())\n",
    "metric_keys = list(config_results[size_type_keys[0]][size_keys[0]].keys())\n",
    "\n",
    "for metric_key in metric_keys:\n",
    "    if metric_key == 'info':\n",
    "        continue\n",
    "    print('-'*100)\n",
    "    print(f\"Results for {metric_key}\")\n",
    "    print('-'*100)\n",
    "    for size_type_key in size_type_keys:\n",
    "        print(f\"Results for considering model size as: {size_type_key}\")\n",
    "        for size_key in size_keys:\n",
    "            # print('-'*100)\n",
    "            print(f\"Results for {size_key}\")\n",
    "            # print('-'*100)\n",
    "            print(config_results[size_type_key][size_key]['info'])\n",
    "            print(config_results[size_type_key][size_key][metric_key])\n",
    "        print('-'*100)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for GA GNN Area Diff\n",
      "P-value: 1.6072338294258834e-67\n",
      "Reject the null hypothesis\n",
      "Results for GA GNN Cohesion Diff\n",
      "P-value: 3.6901174855714833e-59\n",
      "Reject the null hypothesis\n",
      "Results for GA GNN Coupling Diff\n",
      "P-value: 1.6293203211154507e-19\n",
      "Reject the null hypothesis\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def get_ttest(values):\n",
    "    t_statistic, p_value = stats.ttest_1samp(values, 0, alternative='greater')\n",
    "\n",
    "    # Print the p-value\n",
    "    print(f\"P-value: {p_value}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Reject the null hypothesis\")\n",
    "    else:\n",
    "        if t_statistic < 0:\n",
    "            print(\"GA is better than GNN\")\n",
    "        else:\n",
    "            print(\"GNN is better than GA\")\n",
    "\n",
    "final_result['GA GNN Area Diff'] = final_result['max_ga_area'] - final_result['Max Hypervolume GNN']\n",
    "final_result['GA GNN Cohesion Diff'] = final_result['Max Cohesion'] - final_result['Max Cohesion (GNNs)']\n",
    "final_result['GA GNN Coupling Diff'] = final_result['Min Coupling'] - final_result['Min Coupling (GNNs)']\n",
    "\n",
    "diff_keys = ['GA GNN Area Diff', 'GA GNN Cohesion Diff', 'GA GNN Coupling Diff']\n",
    "for diff_key in diff_keys:\n",
    "    print(f\"Results for {diff_key}\")\n",
    "    get_ttest(final_result[diff_key].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
